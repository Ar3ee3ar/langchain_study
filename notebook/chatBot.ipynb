{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f21ede0",
   "metadata": {},
   "source": [
    "# Langchain : Chat bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa9e8d",
   "metadata": {},
   "source": [
    "## Install required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca117990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-core in d:\\personal_proj\\.conda\\lib\\site-packages (0.3.74)\n",
      "Collecting langgraph==0.5.4\n",
      "  Downloading langgraph-0.5.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from langgraph==0.5.4) (2.1.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.6.0,>=0.5.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from langgraph==0.5.4) (0.5.1)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in d:\\personal_proj\\.conda\\lib\\site-packages (from langgraph==0.5.4) (0.1.72)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in d:\\personal_proj\\.conda\\lib\\site-packages (from langgraph==0.5.4) (2.11.7)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from langgraph==0.5.4) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph==0.5.4) (1.10.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in d:\\personal_proj\\.conda\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.5.4) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in d:\\personal_proj\\.conda\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.5.4) (3.10.18)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-core) (0.4.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-core) (4.14.0)\n",
      "Requirement already satisfied: packaging>=23.2 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\personal_proj\\.conda\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: anyio in d:\\personal_proj\\.conda\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.5.4) (4.9.0)\n",
      "Requirement already satisfied: certifi in d:\\personal_proj\\.conda\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.5.4) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\personal_proj\\.conda\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.5.4) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\personal_proj\\.conda\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.5.4) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\personal_proj\\.conda\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.5.4) (0.16.0)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\personal_proj\\.conda\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (2.32.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from pydantic>=2.7.4->langgraph==0.5.4) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\personal_proj\\.conda\\lib\\site-packages (from pydantic>=2.7.4->langgraph==0.5.4) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from pydantic>=2.7.4->langgraph==0.5.4) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\personal_proj\\.conda\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\personal_proj\\.conda\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\personal_proj\\.conda\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.5.4) (1.3.1)\n",
      "Downloading langgraph-0.5.4-py3-none-any.whl (143 kB)\n",
      "Installing collected packages: langgraph\n",
      "  Attempting uninstall: langgraph\n",
      "    Found existing installation: langgraph 0.5.0\n",
      "    Uninstalling langgraph-0.5.0:\n",
      "      Successfully uninstalled langgraph-0.5.0\n",
      "Successfully installed langgraph-0.5.0\n",
      "Requirement already satisfied: langchain-openai in d:\\personal_proj\\.conda\\lib\\site-packages (0.3.29)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-openai) (0.3.74)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-openai) (1.99.9)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.4.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (4.14.0)\n",
      "Requirement already satisfied: packaging>=23.2 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in d:\\personal_proj\\.conda\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\personal_proj\\.conda\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in d:\\personal_proj\\.conda\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\personal_proj\\.conda\\lib\\site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\personal_proj\\.conda\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\personal_proj\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\personal_proj\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\personal_proj\\.conda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\personal_proj\\.conda\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\personal_proj\\.conda\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\personal_proj\\.conda\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\personal_proj\\.conda\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\personal_proj\\.conda\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\personal_proj\\.conda\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
      "Requirement already satisfied: colorama in d:\\personal_proj\\.conda\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.86.0->langchain-openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-core langgraph==0.5.4\n",
    "!pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc500b",
   "metadata": {},
   "source": [
    "## LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dd5fd4",
   "metadata": {},
   "source": [
    "Inspect what going on inside your chain or agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02c34705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from secret.secret_key import openapi_key, serapapi_key, second_openai_key, langsmith_key\n",
    "os.environ['OPENAI_API_KEY'] = second_openai_key\n",
    "os.environ['LANGSMITH_TRACING'] = \"true\"\n",
    "os.environ['LANGSMITH_API_KEY'] = langsmith_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d172aecd",
   "metadata": {},
   "source": [
    "## Get start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466d6c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e96256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi, Zezar! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 12, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-C3iIEleTIzuJniZzjNVqt9h2YzjnE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--dd247b52-175c-4d0b-9442-c34dc6bde7a3-0', usage_metadata={'input_tokens': 12, 'output_tokens': 12, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Manually call ChatModel\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Zezar\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01588b",
   "metadata": {},
   "source": [
    "Normally model on its own not have any concept of state. So, by asking question relate to previous conversation. Model can't answer correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ca01e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but I don't have access to personal information unless you share it with me. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 11, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-C3iIGTgjW6HWbx8AE7wKfLdzv67yF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--d2538966-a8f8-465f-834e-219e5222183c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 25, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7108eba2",
   "metadata": {},
   "source": [
    "We need to pass the entire <b>conversation history</b> in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "947ac56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Zezar! How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 35, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-C3iIHUg6jemr7Ko0DtIDE5s3NbCvE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--49a3701f-9a63-47f4-bf71-f5dfc1cce6b2-0', usage_metadata={'input_tokens': 35, 'output_tokens': 13, 'total_tokens': 48, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Zezar\"),\n",
    "        AIMessage(content=\"Hello Zezar! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd9210",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817c84b",
   "metadata": {},
   "source": [
    "implements a built-in persistence layer, can support multiple conversation turns. Wrapping our chat model in LangGraph allows us to automatically persist the message history. LangGraph comes with simple in-memory checkpointer (can use different persistence backends - SQLite or Postgres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "418c50ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10b78e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "# View\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b3521",
   "metadata": {},
   "source": [
    "we now need to create a <b>config</b> that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to incluse a <b>thread_id</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac8d26bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c8d83",
   "metadata": {},
   "source": [
    "This enable app to supprt multiple conversation threads with a single application (common requirement for application with multiple users.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aba786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Zezar! Great to meet you! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Zezar.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config) # include config to gether history\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53cd73d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Zezar! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5015403",
   "metadata": {},
   "source": [
    "If we change the config to reference a different <b>thread_id</b>. the conversation will start fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0063f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal data about users unless it's shared with me during our conversation. So, I don't know your name. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469c015",
   "metadata": {},
   "source": [
    "We can always go back to original conversations (because persisting in a database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cf37164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Zezar. If there's anything specific you'd like to chat about or ask, feel free!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee9c8d",
   "metadata": {},
   "source": [
    "For async support, update the <b>call_model</b> node to be async function and use <b>.ainvoke</b> when invoking the applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251dace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't know your name. If you'd like, you can share it with me!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before: (because define new Graph even though use same config. Program can't get chat history )\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58861d75",
   "metadata": {},
   "source": [
    "## Prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cfd31b",
   "metadata": {},
   "source": [
    "turn raw user information into a format that the LLM can work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12c4b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e101caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state) # add template\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4e860d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy, Jim! Welcome aboard me ship! What be ye seekin' on this fine day? Arrr!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2be52d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ye be called Jim, if I be readin’ me notes right! What else can I do fer ye, matey? Arrr!\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d97d1",
   "metadata": {},
   "source": [
    "- add <b>language</b> input to prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82926d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acd3d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "# update application's state\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d002013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "¡Hola, Bob! ¿Cómo puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "language = \"Spanish\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8612c6",
   "metadata": {},
   "source": [
    "The entire state is persisted, so we can omit parameters like <b>language</b> if no changes are desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65a989b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tu nombre es Bob.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43269dbe",
   "metadata": {},
   "source": [
    "## Managing Conversation History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a38f63",
   "metadata": {},
   "source": [
    "Important concept of chatbot is how to manage conversation history. If let unmanaged, the list of messages will grow unbounded and potentially overflow the context window of LLM. So, it important to add limit size of messages you are passing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec689a",
   "metadata": {},
   "source": [
    "We can do this by adding a step in front of the prompt that modifies the <b>messages</b> key and then wrap that new chain in the Message history class. (before prompt template but after load previous messages from Message history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "43086178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "# managing list of messages\n",
    "trimmer = trim_messages( # reduce how many message send to model\n",
    "    max_tokens=91, # How many tokens we want to keep\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5535b681",
   "metadata": {},
   "source": [
    "need to run the trimmer before pass the <b>messages</b> input to our prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1477881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"]) # add trimmer here\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt) # before prompt here\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0ed2c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name. If you'd like to share it, feel free!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)] # load previous messages from Message history\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "795deb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}, id='4103a7e3-d2ac-4a81-bec7-8b02b66097b3'),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}, id='f08fdf43-1112-4444-a2cb-a82e1503f2bf'),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}, id='5e133866-7f81-4bd6-a1ae-ce67dbbcc46f'),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}, id='48556773-11af-47a2-b563-b072d124894f'),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}, id='151bea29-403a-417d-b8b7-6dfb94c47e61'),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}, id='e796614a-3a34-4060-8375-c6dfe9754271'),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}, id='eb67b07c-3e5d-4645-ac49-f0c593acad5c'),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}, id='0dfb4288-3613-497d-9714-eb7efcf998c4'),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}, id='cfb52386-1fd2-46c1-88ed-3064dbeef0d1'),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}, id='a97648b1-cde6-4321-9d42-1b381f2084a2'),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={}, id='3ae7ca04-7f0a-4229-a2e5-13b18db54b7f'),\n",
       " HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='97dc8a3f-4960-4b81-9bd1-ec0c6b2de231')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef95c20",
   "metadata": {},
   "source": [
    "if ask about information that is within the last few messages, it remembers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e1e58905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked what 2 + 2 equals.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59dd95",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ecb006",
   "metadata": {},
   "source": [
    "LLM can take a while to respond and to improve UX the one thing that most applications do is stream back each token as it is generated. This allow user to see progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "109a5254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hi| Todd|!| Here|’s| a| brief| overview| of| the| history| of| America|:\n",
      "\n",
      "|**|Pre|-Col|umb|ian| Era|:**| Before| European| exploration|,| the| Americas| were| inhabited| by| various| Indigenous| peoples| with| diverse| cultures| and| societies|.| Not|able| civilizations| included| the| Maya|,| Az|tec|,| and| In|ca| in| Central| and| South| America|,| as| well| as| various| tribes| across| North| America|.\n",
      "\n",
      "|**|European| Exploration| and| Colon|ization| (|149|2| -| |160|0|s|):|**| Christopher| Columbus| arrived| in| the| Caribbean| in| |149|2|,| marking| the| beginning| of| European| colon|ization|.| Following| Columbus|,| other| explorers| like| Hern|án| Cort|és| and| Francisco| P|izar|ro| conquered| large| em|pires| in| the| Americas|.| The| Spanish|,| French|,| and| English| established| colonies|,| leading| to| significant| changes| in| the| Indigenous| way| of| life|.\n",
      "\n",
      "|**|Colon|ial| America| (|160|7| -| |177|6|):|**| The| English| established| their| first| permanent| colony| at| Jam|estown|,| Virginia|,| in| |160|7|.| Over| the| next| century|,| thirteen| colonies| were| established| along| the| Atlantic| coast|.| T|ensions| grew| between| the| colonies| and| Britain|,| fueled| by| issues| like| taxation| without| representation| and| British| control|.\n",
      "\n",
      "|**|American| Revolution| (|177|5| -| |178|3|):|**| Fr|ustr|ation| with| British| rule| led| to| the| American| Revolution|.| Key| events| included| the| Boston| Tea| Party| and| battles| at| Lexington| and| Concord|.| In| |177|6|,| the| Declaration| of| Independence| was| adopted|,| and| after| several| years| of| conflict|,| the| colonies| achieved| victory| in| |178|3|,| culminating| in| the| Treaty| of| Paris|.\n",
      "\n",
      "|**|Formation| of| a| New| Nation| (|178|3| -| |181|5|):|**| Following| independence|,| the| United| States| established| its| Constitution| in| |178|7|,| creating| a| federal| government|.| The| Bill| of| Rights| was| added| in| |179|1|.| The| War| of| |181|2| against| Britain| solid|ified| national| identity| but| also| revealed| internal| divisions|.\n",
      "\n",
      "|**|Expansion| and| Conflict| (|180|0|s|):|**| The| |19|th| century| saw| significant| territorial| expansion|,| including| the| Louisiana| Purchase| in| |180|3| and| the| west|ward| movement| manifest|ing| in| the| concept| of| Manifest| Destiny|.| This| period| included| conflicts| with| Indigenous| peoples| and| the| Mexican|-American| War| (|184|6|-|184|8|).\n",
      "\n",
      "|**|Civil| War| and| Reconstruction| (|186|1| -| |187|7|):|**| Rising| tensions| over| slavery| between| Northern| and| Southern| states| led| to| the| Civil| War|.| Abraham| Lincoln|'s| presidency| (|186|1|-|186|5|)| was| marked| by| the| struggle| to| preserve| the| Union|.| The| war| ended| in| |186|5|,| followed| by| Reconstruction| efforts| to| integrate| formerly| ensl|aved| people| into| society|,| which| faced| significant| resistance| and| backlash|.\n",
      "\n",
      "|**|Industrial|ization| and| Immigration| (|late| |19|th| -| early| |20|th| century|):|**| The| U|.S|.| underwent| rapid| industrial|ization|,| leading| to| urban|ization| and| waves| of| immigration|.| This| brought| cultural| diversity| but| also| labor| challenges| and| social| tensions|,| including| the| labor| movement| and| the| rise| of| social| reform| movements|.\n",
      "\n",
      "|**|World| Wars| and| the| Great| Depression| (|191|4| -| |194|5|):|**| The| U|.S|.| entered| World| War| I| in| |191|7| and| played| a| pivotal| role| in| the| outcome|.| The| |192|0|s| were| characterized| by| prosperity|,| but| the| Great| Depression| beginning| in| |192|9| caused| widespread| economic| hardship|.| The| U|.S|.| entered| World| War| II| after| the| attack| on| Pearl| Harbor| in| |194|1|,| ultimately| becoming| a| world| super|power| post|-war|.\n",
      "\n",
      "|**|Cold| War| Era| (|194|7| -| |199|1|):|**| Following| WWII|,| the| U|.S|.| entered| into| a| geopolitical| struggle| with| the| Soviet| Union|,| known| as| the| Cold| War|.| This| era| involved| various| conflicts| around| the| globe|,| including| the| Korean| War| and| the| Vietnam| War|,| as| well| as| significant| domestic| changes| regarding| civil| rights| movements|.\n",
      "\n",
      "|**|Modern| America| (|199|1| -| present|):|**| The| Cold| War| ended| in| |199|1|,| leading| to| changes| in| American| foreign| policy| and| a| focus| on| globalization|.| The| |21|st| century| has| seen| challenges| such| as| the| September| |11| attacks| in| |200|1|,| economic| recess|ions|,| political| polarization|,| and| social| movements| advocating| for| issues| like| racial| equality|,| climate| change|,| and| healthcare|.\n",
      "\n",
      "|This| overview| covers| major| events| in| American| history|,| but| there| are| countless| stories| and| details| that| add| depth| to| the| narrative|.| If| you'd| like| to| know| more| about| a| specific| period| or| topic|,| feel| free| to| ask|!||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a history of America.\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
